---
title: Schedule
layout: default
nav_order: 2
---


---
<style>
table, th, td {
  border: 1px solid black;
  border-collapse: collapse;
}
</style>

<table>
    <tr>
        <th>Date</th>
        <th>Topic</th>
        <th>References</th>
    </tr>
    <tr>
        <td colspan=3 style="background: #a2a8d3; text-align: center"> Week 1</td>
    </tr>
    <tr>
        <td>Aug 26</td>
        <td>Logistics & Introduction (I)</td>
        <td><a href="https://arxiv.org/abs/1912.04977">Advances and Open Problems in Federated Learning</a><br/><a href="https://arxiv.org/abs/1908.07873">Federated Learning: Challenges, Methods, and Future Directions</a></td>
    </tr>
        <tr>
        <td>Aug 28</td>
        <td>Introduction (II)</td>
        <td><a href="https://drive.google.com/file/d/1QGY2Zytp9XRSu95fX2lCld8DwfEdcHCG/view">Federated Learning Tutorial (NeurIPS 2020)</a><br/><a href="https://federated.withgoogle.com">Google Blog</a><br><a href="https://cs.stanford.edu/people/widom/paper-writing.html">Tips for Writing Technical Papers</a></td>
    </tr>
        <tr>
        <td colspan="3" style="background: #a2a8d3; text-align: center"> Week 2</td>
    </tr>
        <tr>
        <td>Sep 2 </td>
        <td colspan="2" style="text-align: center"><i><b>Labor Day (no class)</b></i></td>   
    </tr>
        <tr>
        <td>Sep 4</td>
        <td><a href="https://arxiv.org/pdf/1602.05629">Communication-Efficient Learning of Deep Networks from Decentralized Data</a><br/><a href="https://arxiv.org/pdf/1902.01046">Towards Federated Learning at Scale: System Design</a></td>
        <td></td>
    </tr>
    <tr>
        <td colspan="3" style="background: #a2a8d3; text-align: center"> Week 3</td>
    </tr>
        <tr>
        <td>Sep 9</td>
        <td><a href="https://arxiv.org/pdf/1812.06127">Federated Optimization in Heterogeneous Networks</a><br/><a href="https://arxiv.org/pdf/1910.06378">SCAFFOLD: Stochastic Controlled Averaging for Federated Learning</a></td>
        <td></td>
    </tr>
        <tr>
        <td>Sep 11</td>
        <td><a href="https://arxiv.org/abs/2003.00295">Adaptive Federated Optimization</a><br/><a href="https://arxiv.org/abs/2007.07481">Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization</a></td>
        <td></td>
    </tr>
        <tr>
        <td colspan="3" style="background: #a2a8d3; text-align: center"> Week 4</td>
    </tr>
        <tr>
        <td>Sep 16</td>
        <td><a href="https://arxiv.org/abs/1806.00582">Federated Learning with Non-IID Data</a><br/><a href="https://arxiv.org/abs/1805.09767">Local SGD Converges Fast and Communicates Little</a></td>
        <td></td>
    </tr>
        <tr>
        <td>Sep 18</td>
        <td><a href="https://arxiv.org/pdf/2002.06440">Federated Learning with Matched Averaging</a><br/><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Model-Contrastive_Federated_Learning_CVPR_2021_paper.pdf">Model-Contrastive Federated Learning</a></td>
        <td></td>
    </tr>
        <tr>
        <td colspan="3" style="background: #a2a8d3; text-align: center"> Week 5</td>
    </tr>
        <tr>
        <td>Sep 23</td>
        <td><a href="https://arxiv.org/abs/1610.05492">Federated Learning: Strategies for Improving Communication Efficiency</a><br/><a href="https://arxiv.org/abs/1709.08765">Network Topology and Communication-Computation Tradeoffs in Decentralized Optimization</a></td>
        <td></td>
    </tr>
        <tr>
        <td>Sep 25</td>
        <td><a href="https://arxiv.org/abs/1808.07576">Cooperative SGD: A unified Framework for the Design and Analysis of Communication-Efficient SGD Algorithms</a><br/><a href="https://proceedings.mlr.press/v97/haddadpour19a.html">Trading Redundancy for Communication: Speeding up Distributed SGD for Non-convex Optimization</a></td>
        <td></td>
    </tr>
    <tr>
        <td colspan="3" style="background: #a2a8d3; text-align: center"> Week 6</td>
    </tr>
        <tr>
        <td>Sep 30</td>
        <td><a href="https://arxiv.org/abs/1909.05350">The Error-Feedback Framework: Better Rates for SGD with Delayed Gradients and Compressed Communication</a><br/><a href="https://arxiv.org/abs/1810.08313">Adaptive Communication Strategies to Achieve the Best Error-Runtime Trade-off in Local-Update SGD</a></td>
        <td></td>
    </tr>
        <tr>
        <td>Oct 2</td>
        <td><a href="https://arxiv.org/abs/1912.12844">Variance Reduced Local SGD with Lower Communication Complexity</a><br/><a href="https://arxiv.org/abs/2207.09653">FedDM: Iterative Distribution Matching for Communication-Efficient Federated Learning</a></td>
        <td></td>
    </tr>
    <tr>
        <td colspan="3" style="background: #a2a8d3; text-align: center"> Week 7</td>
    </tr>
        <tr>
        <td>Oct 7</td>
        <td><a href="https://arxiv.org/abs/2008.03371">LotteryFL: Personalized and Communication-Efficient Federated Learning with Lottery Ticket Hypothesis on Non-IID Datasets</a><br/><a href="https://arxiv.org/abs/2301.09604">FedExP: Speeding Up Federated Averaging via Extrapolation</a></td>
        <td></td>
    </tr>
        <tr>
        <td>Oct 9</td>
        <td><a href="https://arxiv.org/abs/1902.11175">One-Shot Federated Learning</a><br/><a href="https://ieeexplore.ieee.org/document/9354925">Toward resource-efficient federated learning in mobile edge computing</a></td>
        <td></td>
    </tr>
    <tr>
        <td colspan="3" style="background: #a2a8d3; text-align: center"> Week 8</td>
    </tr>
        <tr>
        <td>Oct 14</td>
        <td colspan="2" style="background: #e7eaf6; text-align: center"><i><b>Project Proposal Presentations</b></i></td>
    </tr>
        <tr>
        <td>Oct 16</td>
        <td colspan="2" style="background: #e7eaf6; text-align: center"><i><b>Project Proposal Presentations</b></i></td>
    </tr>
    <tr>
        <td colspan="3" style="background: #a2a8d3; text-align: center"> Week 9</td>
    </tr>
        <tr>
        <td>Oct 21</td>
        <td><a href="https://arxiv.org/abs/1909.12326">Model Pruning Enables Efficient Federated Learning on Edge Devices</a><br/><a href="https://arxiv.org/abs/1911.02417">Energy Efficient Federated Learning Over Wireless Communication Networks</a></td>
        <td></td>
    </tr>
        <tr>
        <td>Oct 23</td>
        <td><a href="https://arxiv.org/abs/2010.01264">HeteroFL: Computation and Communication Efficient Federated Learning for Heterogeneous Clients</a><br/><a href="https://arxiv.org/abs/2408.08977">FedFQ: Federated Learning with Fine-Grained Quantization</a></td>
        <td></td>
    </tr>
    <tr>
        <td colspan="3" style="background: #a2a8d3; text-align: center"> Week 10</td>
    </tr>
        <tr>
        <td>Oct 28</td>
        <td><a href="https://dl.acm.org/doi/pdf/10.1145/3485730.3485929">FedMask: Joint Computation and Communication-Efficient Personalized Federated Learning via Heterogeneous Masking</a><br/><a href="https://arxiv.org/abs/1911.02134">Asynchronous Online Federated Learning for Edge Devices with Non-IID Data</a></td>
        <td></td>
    </tr>
        <tr>
        <td>Oct 30</td>
        <td><a href="https://arxiv.org/abs/1911.02134">Enhancing Convergence in Federated Learning: A Contribution-Aware Asynchronous Approach</a><br/><a href="https://www.usenix.org/system/files/osdi21-lai.pdf">Oort: Efficient Federated Learning via Guided Participant Selection</a></td>
        <td></td>
    </tr>
    <tr>
        <td colspan="3" style="background: #a2a8d3; text-align: center"> Week 11</td>
    </tr>
        <tr>
        <td>Nov 4</td>
        <td><a href="http://proceedings.mlr.press/v139/li21h/li21h.pdf">Ditto: Fair and Robust Federated Learning Through Personalization</a><br/><a href="https://dl.acm.org/doi/pdf/10.1145/3133956.3133982">Practical Secure Aggregation for Privacy-Preserving Machine Learning</a></td>
        <td></td>
    </tr>
        <tr>
        <td>Nov 6</td>
        <td><a href="https://arxiv.org/abs/1807.00459">How To Backdoor Federated Learning</a><br/><a href="https://arxiv.org/abs/1911.07963">Can You Really Backdoor Federated Learning?</a></td>
        <td></td>
    </tr>
    <tr>
        <td colspan="3" style="background: #a2a8d3; text-align: center"> Week 12</td>
    </tr>
        <tr>
        <td>Nov 11</td>
        <td><a href="https://arxiv.org/abs/2007.05084">Attack of the Tails: Yes, You Really Can Backdoor Federated Learning</a><br/><a href="https://arxiv.org/abs/2112.00059">Evaluating Gradient Inversion Attacks and Defenses in Federated Learning</a></td>
        <td></td>
    </tr>
        <tr>
        <td>Nov 13</td>
        <td><a href="https://arxiv.org/abs/1911.00222">Federated Learning with Differential Privacy: Algorithms and Performance Analysis</a><br/><a href="https://dl.acm.org/doi/pdf/10.1145/3458864.3466628">PPFL: Privacy-preserving Federated Learning with Trusted Execution Environments</a></td>
        <td></td>
    </tr>
    <tr>
        <td colspan="3" style="background: #a2a8d3; text-align: center"> Week 13</td>
    </tr>
        <tr>
        <td>Nov 18</td>
        <td><a href="https://arxiv.org/abs/1611.04482">Practical Secure Aggregation for Federated Learning on User-Held Data</a><br/><a href="https://www.usenix.org/system/files/atc20-zhang-chengliang.pdf">BatchCrypt: Efficient Homomorphic Encryption for Cross-Silo Federated Learning</a></td>
        <td></td>
    </tr>
        <tr>
        <td>Nov 20</td>
        <td><a href="https://arxiv.org/abs/2305.05644">Towards Building the Federated GPT: Federated Instruction Tuning</a><br/><a href="https://arxiv.org/abs/2401.06432">Heterogeneous LoRA for Federated Fine-tuning of On-Device Foundation Models</a></td>
        <td></td>
    </tr>
    <tr>
        <td colspan="3" style="background: #a2a8d3; text-align: center"> Week 14</td>
    </tr>
        <tr>
        <td>Nov 25</td>
        <td><a href="https://arxiv.org/abs/2405.10853">The Future of Large Language Model Pre-training is Federated</a><br/><a href="https://arxiv.org/abs/2402.06954">OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning</a></td>
        <td></td>
    </tr>
        <tr>
        <td>Nov 27 </td>
        <td colspan="2" style="text-align: center"><i><b>Thanksgiving (no class)</b></i></td>
    </tr>
     <tr>
        <td colspan="3" style="background: #a2a8d3; text-align: center"> Week 15</td>
    </tr>
        <tr>
        <td>Dec 2</td>
        <td><a href="https://arxiv.org/abs/2310.03150">Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the Ugly</a><br/><a href="https://arxiv.org/abs/2401.10375">Vulnerabilities of Foundation Model Integrated Federated Learning Under Adversarial Threats</a></td>
        <td></td>
    </tr>
        <tr>
        <td>Dec 4</td>
        <td colspan="2" style="background: #e7eaf6; text-align: center"><i><b>Final Project Presentations</b></i></td>
    </tr>
     <tr>
        <td colspan="3" style="background: #a2a8d3; text-align: center"> Week 16</td>
    </tr>
        <tr>
        <td>Dec 9 (Last Day of Classes)</td>
        <td colspan="2" style="background: #e7eaf6; text-align: center"><i><b>Final Project Presentations</b></i></td>
    </tr>
</table>